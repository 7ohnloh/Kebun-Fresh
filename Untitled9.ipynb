{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/7ohnloh/Kebun-Fresh/blob/main/Untitled9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtbHJroJE_xF",
        "outputId": "1fc23734-e87d-44d6-abc8-8cf0a2500015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDj_exYry81X",
        "outputId": "0b72c7f0-4461-4dbd-e745-3b11bc0f20eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##THE CORRECT ONE"
      ],
      "metadata": {
        "id": "5OqgU3Z21Yh4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 1. Membuat sesi Spark\n",
        "spark = SparkSession.builder.appName(\"Personalized Recommendation System\").getOrCreate()\n",
        "\n",
        "# 2. Memuat dataset\n",
        "dataset_path = \"/content/drive/My Drive/User_Purchase_and_Rating.csv\"  # Ganti dengan path dataset Anda\n",
        "df_large = spark.read.csv(dataset_path, header=True, inferSchema=True).dropna().dropDuplicates()\n",
        "\n",
        "# 3. Encoding kolom \"Product_Name\" dan \"Farm_Name\" ke indeks numerik\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "product_indexer = StringIndexer(inputCol=\"Product_Name\", outputCol=\"Product_ID\")\n",
        "farm_indexer = StringIndexer(inputCol=\"Farm_Name\", outputCol=\"Farm_ID\")\n",
        "\n",
        "# Terapkan encoding\n",
        "product_indexed = product_indexer.fit(df_large).transform(df_large)\n",
        "farm_indexed = farm_indexer.fit(product_indexed).transform(product_indexed)\n",
        "\n",
        "# Pastikan kolom Product_ID dan Farm_ID ada\n",
        "encoded_df = farm_indexed.select(\"User_ID\", \"Product_Name\", \"Farm_Name\", \"Rating\", \"Product_ID\", \"Farm_ID\")\n",
        "\n",
        "# 4. Dataset untuk ALS\n",
        "als_data = encoded_df.select(\n",
        "    col(\"User_ID\").cast(\"integer\"),\n",
        "    col(\"Product_ID\").cast(\"integer\"),\n",
        "    col(\"Rating\").cast(\"float\")\n",
        ").cache()\n",
        "\n",
        "# 5. Split data menjadi pelatihan dan pengujian\n",
        "train, test = als_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 6. Melatih model ALS\n",
        "als = ALS(\n",
        "    userCol=\"User_ID\",\n",
        "    itemCol=\"Product_ID\",\n",
        "    ratingCol=\"Rating\",\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "model = als.fit(train)\n",
        "\n",
        "# 7. Evaluasi model (RMSE)\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"Rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "predictions = model.transform(test)  # Memprediksi data pengujian\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Square Error (RMSE) pada data pengujian: {rmse:.4f}\")\n",
        "\n",
        "# 8. Tentukan user yang akan diberikan rekomendasi\n",
        "target_user_id = 1  # Ganti dengan ID user yang diinginkan\n",
        "\n",
        "# 9. Ambil riwayat pembelian user\n",
        "user_history = encoded_df.filter(col(\"User_ID\") == target_user_id).select(\"Product_ID\", \"Farm_ID\").distinct()\n",
        "\n",
        "# 10. Berikan rekomendasi khusus untuk user\n",
        "user_recommendations = model.recommendForUserSubset(\n",
        "    spark.createDataFrame([(target_user_id,)], [\"User_ID\"]),\n",
        "    numItems=5\n",
        ")\n",
        "\n",
        "# 11. Tambahkan nama produk ke rekomendasi\n",
        "product_mapping = encoded_df.select(\"Product_ID\", \"Product_Name\").distinct()\n",
        "\n",
        "user_recommendations = user_recommendations.withColumn(\"recommendations\", explode(col(\"recommendations\")))\n",
        "\n",
        "user_recommendations = user_recommendations.withColumn(\"Product_ID\", col(\"recommendations\").getItem(\"Product_ID\")) \\\n",
        "    .withColumn(\"Rating\", col(\"recommendations\").getItem(\"rating\")) \\\n",
        "    .join(product_mapping, on=\"Product_ID\", how=\"left\") \\\n",
        "    .select(\"User_ID\", \"Product_Name\", \"Rating\", \"Product_ID\")\n",
        "\n",
        "# 12. Tambahkan kombinasi farm dari data asli\n",
        "product_farm_mapping = encoded_df.select(\"Product_ID\", \"Farm_ID\", \"Farm_Name\").distinct()\n",
        "\n",
        "final_recommendations = user_recommendations.join(\n",
        "    product_farm_mapping,\n",
        "    on=\"Product_ID\",\n",
        "    how=\"inner\"  # Menambahkan farm ke produk\n",
        ")\n",
        "\n",
        "# 13. Filter kombinasi unik (produk + farm)\n",
        "final_recommendations = final_recommendations.join(\n",
        "    user_history,\n",
        "    on=[\"Product_ID\", \"Farm_ID\"],\n",
        "    how=\"left_anti\"  # Hanya rekomendasi produk dan farm yang belum ada di riwayat\n",
        ")\n",
        "\n",
        "# 14. Menampilkan hasil rekomendasi\n",
        "print(f\"Rekomendasi untuk User ID {target_user_id}:\")\n",
        "final_recommendations.show(truncate=False)\n",
        "\n",
        "# 15. Menyimpan hasil rekomendasi untuk user\n",
        "final_recommendations.write.parquet(f\"user_{target_user_id}_recommendations2.parquet\", compression=\"snappy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ug5johB0BuI9",
        "outputId": "3d4543d6-b9ae-49c9-eb53-96d5afb6a06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Square Error (RMSE) pada data pengujian: 0.7012\n",
            "Rekomendasi untuk User ID 1:\n",
            "+----------+-------+-------+------------+---------+---------+\n",
            "|Product_ID|Farm_ID|User_ID|Product_Name|Rating   |Farm_Name|\n",
            "+----------+-------+-------+------------+---------+---------+\n",
            "|2         |4.0    |1      |Terong      |3.933017 |Farm A   |\n",
            "|2         |2.0    |1      |Terong      |3.933017 |Farm B   |\n",
            "|2         |1.0    |1      |Terong      |3.933017 |Farm C   |\n",
            "|2         |3.0    |1      |Terong      |3.933017 |Farm D   |\n",
            "|5         |1.0    |1      |Buah Tin    |3.8768957|Farm C   |\n",
            "|5         |0.0    |1      |Buah Tin    |3.8768957|Farm E   |\n",
            "|5         |4.0    |1      |Buah Tin    |3.8768957|Farm A   |\n",
            "|4         |2.0    |1      |Telur       |3.8755577|Farm B   |\n",
            "|4         |3.0    |1      |Telur       |3.8755577|Farm D   |\n",
            "|4         |0.0    |1      |Telur       |3.8755577|Farm E   |\n",
            "|6         |2.0    |1      |Pepaya      |3.7687273|Farm B   |\n",
            "|6         |3.0    |1      |Pepaya      |3.7687273|Farm D   |\n",
            "|6         |1.0    |1      |Pepaya      |3.7687273|Farm C   |\n",
            "|0         |1.0    |1      |Pisang      |3.7420049|Farm C   |\n",
            "|0         |3.0    |1      |Pisang      |3.7420049|Farm D   |\n",
            "+----------+-------+-------+------------+---------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import col, concat_ws, split, explode\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# 1. Create Spark session\n",
        "spark = SparkSession.builder.appName(\"Personalized Recommendation System\").getOrCreate()\n",
        "\n",
        "# 2. Load dataset\n",
        "dataset_path = \"/content/drive/MyDrive/User_Purchase_and_Rating.csv\"  # Replace with your dataset path\n",
        "df_large = spark.read.csv(dataset_path, header=True, inferSchema=True).dropna().dropDuplicates()\n",
        "\n",
        "# 3. Encode \"Product_Name\" and \"Farm_Name\" to numeric indices\n",
        "product_indexer = StringIndexer(inputCol=\"Product_Name\", outputCol=\"Product_ID\")\n",
        "farm_indexer = StringIndexer(inputCol=\"Farm_Name\", outputCol=\"Farm_ID\")\n",
        "\n",
        "# Apply encoding\n",
        "product_indexed = product_indexer.fit(df_large).transform(df_large)\n",
        "farm_indexed = farm_indexer.fit(product_indexed).transform(product_indexed)\n",
        "\n",
        "# 4. Create a unique Farm-Product ID\n",
        "encoded_df = farm_indexed.withColumn(\"Farm_Product_ID\", concat_ws(\"_\", col(\"Product_ID\"), col(\"Farm_ID\")).cast(\"string\"))\n",
        "\n",
        "# 5. Generate numeric index for Farm_Product_ID\n",
        "indexer = StringIndexer(inputCol=\"Farm_Product_ID\", outputCol=\"Farm_Product_Index\")\n",
        "encoded_df = indexer.fit(encoded_df).transform(encoded_df)\n",
        "\n",
        "# 6. Prepare dataset for ALS\n",
        "als_data = encoded_df.select(\n",
        "    col(\"User_ID\").cast(\"integer\"),\n",
        "    col(\"Farm_Product_Index\").cast(\"double\"),  # Use numeric Farm_Product_Index\n",
        "    col(\"Rating\").cast(\"float\")\n",
        ").cache()\n",
        "\n",
        "# 7. Split data into training and testing sets\n",
        "train, test = als_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 8. Train ALS model\n",
        "als = ALS(\n",
        "    userCol=\"User_ID\",\n",
        "    itemCol=\"Farm_Product_Index\",  # Use numeric Farm-Product Index\n",
        "    ratingCol=\"Rating\",\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "model = als.fit(train)\n",
        "\n",
        "# 9. Evaluate model using RMSE\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"Rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "predictions = model.transform(test)  # Predict on test data\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Square Error (RMSE) on test data: {rmse:.4f}\")\n",
        "\n",
        "# 10. Define the target user ID for recommendations\n",
        "target_user_id = 2  # Replace with your target user ID\n",
        "\n",
        "# 11. Get user's purchase history\n",
        "user_history = encoded_df.filter(col(\"User_ID\") == target_user_id).select(\"Product_ID\", \"Farm_ID\").distinct()\n",
        "\n",
        "# 12. Generate recommendations for the user\n",
        "user_recommendations = model.recommendForUserSubset(\n",
        "    spark.createDataFrame([(target_user_id,)], [\"User_ID\"]),\n",
        "    numItems=5\n",
        ")\n",
        "\n",
        "# 13. Explode recommendations and extract fields\n",
        "user_recommendations = user_recommendations.withColumn(\"recommendations\", explode(col(\"recommendations\")))\n",
        "\n",
        "# Extract Farm_Product_Index and Rating from the exploded recommendations\n",
        "user_recommendations = user_recommendations.withColumn(\"Farm_Product_Index\", col(\"recommendations\").getItem(\"Farm_Product_Index\")) \\\n",
        "    .withColumn(\"Rating\", col(\"recommendations\").getItem(\"rating\"))\n",
        "\n",
        "# 14. Split Farm_Product_Index into Product_ID and Farm_ID\n",
        "user_recommendations = user_recommendations.withColumn(\"Product_ID\", split(col(\"Farm_Product_Index\"), \"_\")[0]) \\\n",
        "    .withColumn(\"Farm_ID\", split(col(\"Farm_Product_Index\"), \"_\")[1])\n",
        "\n",
        "# 15. Add product names to recommendations\n",
        "product_mapping = encoded_df.select(\"Product_ID\", \"Product_Name\").distinct()\n",
        "user_recommendations = user_recommendations.join(product_mapping, on=\"Product_ID\", how=\"left\")\n",
        "\n",
        "# 16. Add farm names to recommendations\n",
        "product_farm_mapping = encoded_df.select(\"Product_ID\", \"Farm_ID\", \"Farm_Name\").distinct()\n",
        "final_recommendations = user_recommendations.join(\n",
        "    product_farm_mapping,\n",
        "    on=[\"Product_ID\", \"Farm_ID\"],\n",
        "    how=\"inner\"  # Add farm names to products\n",
        ")\n",
        "\n",
        "# 17. Filter out items already in the user's purchase history\n",
        "final_recommendations = final_recommendations.join(\n",
        "    user_history,\n",
        "    on=[\"Product_ID\", \"Farm_ID\"],\n",
        "    how=\"left_anti\"  # Only recommend new items\n",
        ")\n",
        "\n",
        "# 18. Display final recommendations\n",
        "print(f\"Recommendations for User ID {target_user_id}:\")\n",
        "final_recommendations.select(\"User_ID\", \"Product_Name\", \"Rating\", \"Farm_Name\").show(truncate=False)\n",
        "\n",
        "# 19. Save final recommendations to a Parquet file\n",
        "final_recommendations.write.parquet(f\"user_{target_user_id}_recommendations.parquet\", compression=\"snappy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nn78YEeRJmWA",
        "outputId": "fea9a0e4-8247-442f-ff15-0e6422d270be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Square Error (RMSE) on test data: 0.7177\n",
            "Recommendations for User ID 2:\n",
            "+-------+------------+------+---------+\n",
            "|User_ID|Product_Name|Rating|Farm_Name|\n",
            "+-------+------------+------+---------+\n",
            "+-------+------------+------+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql.functions import col, explode\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# 1. Membuat sesi Spark\n",
        "spark = SparkSession.builder.appName(\"Personalized Recommendation System\").getOrCreate()\n",
        "\n",
        "# 2. Memuat dataset\n",
        "dataset_path = \"/content/drive/MyDrive/User_Purchase_and_Rating.csv\"  # Ganti dengan path dataset Anda\n",
        "df_large = spark.read.csv(dataset_path, header=True, inferSchema=True).dropna().dropDuplicates()\n",
        "\n",
        "# 3. Encoding kolom \"Product_Name\" dan \"Farm_Name\" ke indeks numerik\n",
        "product_indexer = StringIndexer(inputCol=\"Product_Name\", outputCol=\"Product_ID\")\n",
        "farm_indexer = StringIndexer(inputCol=\"Farm_Name\", outputCol=\"Farm_ID\")\n",
        "\n",
        "# Terapkan encoding\n",
        "product_indexed = product_indexer.fit(df_large).transform(df_large)\n",
        "farm_indexed = farm_indexer.fit(product_indexed).transform(product_indexed)\n",
        "\n",
        "# 4. Membuat kombinasi unik Farm-Product ID\n",
        "encoded_df = farm_indexed.withColumn(\"Farm_Product_ID\", concat_ws(\"_\", col(\"Farm_ID\"), col(\"Product_ID\")).cast(\"string\"))\n",
        "\n",
        "# 5. Menggunakan StringIndexer untuk Farm_Product_ID menjadi angka unik\n",
        "indexer = StringIndexer(inputCol=\"Farm_Product_ID\", outputCol=\"Farm_Product_Index\")\n",
        "encoded_df = indexer.fit(encoded_df).transform(encoded_df)\n",
        "\n",
        "# 6. Dataset untuk ALS (menggunakan Farm_Product_Index sebagai item)\n",
        "als_data = encoded_df.select(\n",
        "    col(\"User_ID\").cast(\"integer\"),\n",
        "    col(\"Farm_Product_Index\").cast(\"double\"),  # Farm-Product Combination sebagai item yang numerik\n",
        "    col(\"Rating\").cast(\"float\")\n",
        ").cache()\n",
        "\n",
        "# 7. Split data menjadi pelatihan dan pengujian\n",
        "train, test = als_data.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# 8. Melatih model ALS\n",
        "als = ALS(\n",
        "    userCol=\"User_ID\",\n",
        "    itemCol=\"Farm_Product_Index\",  # Gunakan Farm-Product Index yang numerik\n",
        "    ratingCol=\"Rating\",\n",
        "    coldStartStrategy=\"drop\"\n",
        ")\n",
        "model = als.fit(train)\n",
        "\n",
        "# 9. Evaluasi model (RMSE)\n",
        "evaluator = RegressionEvaluator(\n",
        "    metricName=\"rmse\",\n",
        "    labelCol=\"Rating\",\n",
        "    predictionCol=\"prediction\"\n",
        ")\n",
        "predictions = model.transform(test)  # Memprediksi data pengujian\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(f\"Root Mean Square Error (RMSE) pada data pengujian: {rmse:.4f}\")\n",
        "\n",
        "# 10. Tentukan user yang akan diberikan rekomendasi\n",
        "target_user_id = 1  # Ganti dengan ID user yang diinginkan\n",
        "\n",
        "# 11. Ambil riwayat pembelian user\n",
        "user_history = encoded_df.filter(col(\"User_ID\") == target_user_id).select(\"Farm_ID\", \"Product_ID\").distinct()\n",
        "\n",
        "# 12. Berikan rekomendasi khusus untuk user\n",
        "user_recommendations = model.recommendForUserSubset(\n",
        "    spark.createDataFrame([(target_user_id,)], [\"User_ID\"]),\n",
        "    numItems=5\n",
        ")\n",
        "\n",
        "# 13. Pisahkan rekomendasi menjadi rating dan produk\n",
        "# Explode the recommendations array to get individual items (rating, product)\n",
        "user_recommendations = user_recommendations.withColumn(\"recommendations\", explode(col(\"recommendations\")))\n",
        "\n",
        "# 14. Extract the product and rating from the recommendations\n",
        "user_recommendations = user_recommendations.withColumn(\"Farm_Product_Index\", col(\"recommendations\").getItem(\"Product_Name\"))\n",
        "    .withColumn(\"Rating\", col(\"recommendations\").getItem(\"rating\"))\n",
        "\n",
        "# 15. Join kembali dengan data mapping untuk mendapatkan nama produk dan farm\n",
        "product_mapping = encoded_df.select(\"Farm_Product_Index\", \"Farm_ID\", \"Product_ID\", \"Product_Name\").distinct()\n",
        "user_recommendations = user_recommendations.join(product_mapping, on=\"Farm_Product_Index\", how=\"inner\")\n",
        "\n",
        "# 16. Filter kombinasi unik (produk + farm) yang belum ada di riwayat\n",
        "final_recommendations = user_recommendations.join(\n",
        "    user_history,\n",
        "    on=[\"Farm_ID\", \"Product_ID\"],\n",
        "    how=\"left_anti\"  # Hanya rekomendasi produk dan farm yang belum ada di riwayat\n",
        ")\n",
        "\n",
        "# 17. Menampilkan hasil rekomendasi\n",
        "print(f\"Rekomendasi untuk User ID {target_user_id}:\")\n",
        "final_recommendations.select(\"User_ID\", \"Product_Name\", \"Rating\", \"Farm_ID\").show(truncate=False)\n",
        "\n",
        "# 18. Menyimpan hasil rekomendasi untuk user\n",
        "final_recommendations.write.parquet(f\"user_{target_user_id}_recommendations.parquet\", compression=\"snappy\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "IssJ_ZhG2exN",
        "outputId": "7d62033a-2974-4392-e492-7805c20b5df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Square Error (RMSE) pada data pengujian: 0.7177\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[FIELD_NOT_FOUND] No such struct field `product` in `Farm_Product_Index`, `rating`.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8990d53a7f9c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# 14. Extract the product and rating from the recommendations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0muser_recommendations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_recommendations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Farm_Product_Index\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recommendations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"product\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m     \u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Rating\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recommendations\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetItem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rating\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mwithColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5174\u001b[0m                 \u001b[0mmessage_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"arg_name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"col\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"arg_type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5175\u001b[0m             )\n\u001b[0;32m-> 5176\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexisting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"DataFrame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [FIELD_NOT_FOUND] No such struct field `product` in `Farm_Product_Index`, `rating`."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 1. Muat dataset\n",
        "dataset_path = \"/content/drive/My Drive/Expanded_User_Purchase_and_Rating.csv\"\n",
        "df_large = pd.read_csv(dataset_path).dropna().drop_duplicates()\n",
        "\n",
        "# 2. Encode kolom \"Product_Name\" dan \"Farm_Name\" ke numerik\n",
        "df_large['Product_ID'] = df_large['Product_Name'].astype('category').cat.codes\n",
        "df_large['Farm_ID'] = df_large['Farm_Name'].astype('category').cat.codes\n",
        "\n",
        "# 3. Buat matriks pivot (User x Product) dengan rating\n",
        "user_product_matrix = df_large.pivot_table(\n",
        "    index='User_ID', columns='Product_ID', values='Rating'\n",
        ").fillna(0)\n",
        "\n",
        "# 4. Gunakan SVD untuk reduksi dimensi\n",
        "svd = TruncatedSVD(n_components=10, random_state=42)\n",
        "latent_features = svd.fit_transform(user_product_matrix)\n",
        "\n",
        "# 5. Hitung kemiripan antar-user\n",
        "user_similarity = cosine_similarity(latent_features)\n",
        "\n",
        "# 6. Diversifikasi dan Scoring Farm\n",
        "def diversify_and_score_recommendations(recommendations, user_farm_preference, max_per_farm=2):\n",
        "    \"\"\"\n",
        "    Diversifikasi rekomendasi agar mencakup farm yang berbeda,\n",
        "    dan prioritaskan berdasarkan skor produk.\n",
        "    \"\"\"\n",
        "    diversified = []\n",
        "    farm_count = {}\n",
        "    for product_id, score in recommendations:\n",
        "        farm_name = df_large[df_large['Product_ID'] == product_id]['Farm_Name'].values[0]\n",
        "\n",
        "        # Batasi jumlah rekomendasi per farm\n",
        "        if farm_count.get(farm_name, 0) < max_per_farm:\n",
        "            diversified.append((product_id, score))\n",
        "            farm_count[farm_name] = farm_count.get(farm_name, 0) + 1\n",
        "\n",
        "        if len(diversified) >= 5:  # Batasi ke top 5\n",
        "            break\n",
        "\n",
        "    return diversified\n",
        "\n",
        "# 7. Rekomendasi untuk user tertentu\n",
        "target_user_id = 3\n",
        "target_user_index = user_product_matrix.index.get_loc(target_user_id)\n",
        "\n",
        "# Cari user paling mirip\n",
        "similar_users = user_similarity[target_user_index].argsort()[::-1][1:]  # Exclude user itu sendiri\n",
        "\n",
        "# Ambil produk yang telah dibeli user target\n",
        "purchased_products = set(\n",
        "    df_large[df_large['User_ID'] == target_user_id]['Product_ID'].values\n",
        ")\n",
        "\n",
        "# Cari rekomendasi produk dari user yang mirip\n",
        "recommendations = []\n",
        "for similar_user_index in similar_users:\n",
        "    similar_user_id = user_product_matrix.index[similar_user_index]\n",
        "    similar_user_products = set(\n",
        "        df_large[df_large['User_ID'] == similar_user_id]['Product_ID'].values\n",
        "    )\n",
        "    # Produk yang belum dibeli oleh target user\n",
        "    new_products = similar_user_products - purchased_products\n",
        "    for product_id in new_products:\n",
        "        avg_rating = df_large[df_large['Product_ID'] == product_id]['Rating'].mean()\n",
        "        recommendations.append((product_id, avg_rating))\n",
        "\n",
        "# Urutkan berdasarkan skor produk (rating rata-rata)\n",
        "recommendations = sorted(recommendations, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "# Diversifikasi rekomendasi berdasarkan farm\n",
        "user_farm_preference = df_large[df_large['User_ID'] == target_user_id]['Farm_Name'].value_counts().to_dict()\n",
        "diversified_recommendations = diversify_and_score_recommendations(recommendations, user_farm_preference)\n",
        "\n",
        "# Ambil detail produk yang direkomendasikan\n",
        "recommended_products = df_large[\n",
        "    df_large['Product_ID'].isin([r[0] for r in diversified_recommendations])\n",
        "][['Product_ID', 'Product_Name', 'Farm_Name']].drop_duplicates()\n",
        "\n",
        "# 8. Evaluasi Model\n",
        "precision = len(set([r[0] for r in diversified_recommendations]) & purchased_products) / len(diversified_recommendations)\n",
        "recall = len(set([r[0] for r in diversified_recommendations]) & purchased_products) / len(purchased_products)\n",
        "f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"Precision@5: {precision:.4f}\")\n",
        "print(f\"Recall@5: {recall:.4f}\")\n",
        "print(f\"F1-Score@5: {f1_score:.4f}\")\n",
        "\n",
        "# 9. Tampilkan hasil rekomendasi\n",
        "print(f\"Rekomendasi untuk User ID {target_user_id}:\")\n",
        "print(recommended_products)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "7szAqcD6tYlx",
        "outputId": "e93912b2-a97a-4d47-f6eb-72f8d2ead369"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "n_components(10) must be <= n_features(9).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-ce545b3f9f9e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# 4. Gunakan SVD untuk reduksi dimensi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0msvd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mlatent_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_product_matrix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# 5. Hitung kemiripan antar-user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/decomposition/_truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"randomized\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    241\u001b[0m                     \u001b[0;34mf\"n_components({self.n_components}) must be <=\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m                     \u001b[0;34mf\" n_features({X.shape[1]}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: n_components(10) must be <= n_features(9)."
          ]
        }
      ]
    }
  ]
}